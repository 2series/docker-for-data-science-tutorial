{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talk Recommender - Pycon 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 32 tuotorials, 12 sponsor workshops, 16 talks at the education summit, and 95 talks at the main conference - Pycon has a lot to offer. Reading through all the talk descriptions and filtering out the ones that you should go to is a tedious process. But not anymore.\n",
    "\n",
    "## Introducing TalkRecommender\n",
    "Talk recommender is a recommendation system that recommends talks from this year's Pycon based on the ones that you went to last year.  This way you don't waste any time preparing a schedule and get to see the talks that matter the most to you! \n",
    "\n",
    "As shown in the demo, the users are asked to label previous year's talks into two categories - the one that they went to in person, and the ones they watched later online. Talk Recommender uses those labels to predict talks from this year that will be interesing to them. \n",
    "\n",
    "We will be using [`pandas`](https://pandas.pydata.org/) abd [`scikit-learn`](http://scikit-learn.org/) to build and the model.\n",
    "\n",
    "*Remember to click on Save and Checkpoint from the File menu to save changes you made to the notebook* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise A: Load the data\n",
    "The data directory contains the snapshot of one such user's labeling - lets load that up and start with our analysis! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lrt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv('data/talks.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a brief description of the interesting fields.\n",
    "\n",
    "variable | description  \n",
    "------|------|\n",
    "`title`|Title of the talk\n",
    "`description`|Description of the talk\n",
    "`year`|Is it a `2017` talk or `2018`  \n",
    "`label`|`1` indicates the user preferred seeing the talk in person,<br> `0` indicates they would schedule it for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the 2017 talk descriptions that were labeled by the user for watching in person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.label==1][['description', 'label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise B: Feature Extraction\n",
    "In this step we build the feature set by tokenization, counting and normalization of the bi-grams from the text descriptions of the talk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n",
    "vectorized_text = vectorizer.fit_transform(df['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the `vectorized_text` into two parts - the 2017 talks will be used for training and the 2018 talks will we used for predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[df.year == 2017]['label']\n",
    "count_labeled = len(df[df.year == 2017])\n",
    "vectorized_text_labeled = vectorized_text[:count_labeled]\n",
    "vectorized_text_predict = vectorized_text[count_labeled:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise C: Split into Training and Testing Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we split our data into training set and testing set. This allows us to do cross validation and avoid overfitting. Use the `train_test_split` method from `sklearn.model_selection` to split the `vectorized_text_labeled` into training and testing set with the test size as one third of the size of the labeled.\n",
    "\n",
    "[Here](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) is the documentation for the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_text_labeled, labels, test_size=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise D: Train the model\n",
    "Finally we get to the stage for training the model. We are going to use a linear support vector machine. And check its accuracy by using the `classification_report` function. Note that we have not done any parameter tuning yet, so your model might not give you the best results. Feel free to tweak the parameters or use a different model to get a better result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "report = sklearn.metrics.classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise E: Make Predictions\n",
    "Use the model to predict which 2018 talks the user should go to. Print out the talk descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_talks_vector = classifier.predict(vectorized_text_predict)\n",
    "df_2018 = df[df.year==2018]\n",
    "# Offset the rows by count_labeled as\n",
    "predicted_talks_indexes = predicted_talks_vector.nonzero()[0] + count_labeled\n",
    "df_2018.loc[predicted_talks_indexes][['id', 'description', 'presenters', 'title', 'location']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise F: Expose it as a service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have pieces of the code ready, copy them together into the `model.py` file located in this folder, and rebuild your docker image. Copy the code from the above cells into the body of the `prediction` function.\n",
    "\n",
    "Lets rebuild the docker image and start an new container.\n",
    "```\n",
    "docker stop <container_name>\n",
    "docker build -t recommender .\n",
    "docker run -p 8888:8888 -p 9000:9000 recommender\n",
    "```\n",
    "\n",
    "The `api.py` file in this directory is a flask app that makes call to the `model.py` module and exposes the model built in the previous steps as a service. In order to start the flask server, open a new terminal and run the following command:\n",
    "\n",
    "```\n",
    "docker exec (docker ps -ql) python api.py\n",
    "```\n",
    "Where `docker ps -ql` queries to get the container id of the last conatainer that was created in your system.\n",
    "\n",
    "Finally go to http://0.0.0.0:9000/predict to see the talks that were recommended for this user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise G: Expose it as a service\n",
    "\n",
    "Finally we do not have to retrain our model anytime we have to make predictions. In most real life data science applications, the training phase is a time consuming proecss. We would seaprately train and serialize the model which is then exposed through the api to make the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "with open('talk_recommender.pkl', 'wb') as f:\n",
    "    joblib.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `joblib.load` function to read the `classifier` back from the `talk_recommender.pkl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
